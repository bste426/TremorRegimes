#pip install git+https://github.com/sevamoo/SOMPY.git#egg=SOMPYimport matplotlib.pylab as pltimport globimport sompy as sompyimport pandas as pdimport numpy as npimport pickleimport scipy.signal as ssimport datetimefrom statistics import multimodefrom pandas.plotting import register_matplotlib_convertersimport matplotlib.dates as mdatesimport loggingimport osimport csvfrom matplotlib.ticker import MaxNLocatorimport matplotlib.animation as aniimport matplotlib.pyplot as pplimport joblibimport seaborn as snsfrom matplotlib.colors import LogNormfrom inspect import getfile, currentframefrom datetime import datetime, timedelta, dateimport jsonpickle, jsonfrom sklearn.linear_model import LinearRegressionfrom sklearn.preprocessing import PolynomialFeatures#### SETTINGS ##### GENERAL SETTINGS# Are you reading the matrices from an external harddrive?extdrive = False#Name of volcano, station:volcano = 'Redoubt'# Augustine Redoubt Whakaari Vulcanostation = 'RDT'# AUL AUI AUW AUH   REF RSE RDT   WIZ   IVGP# What time-period would you like to train the SOM with? (train data)startdate = '2008-01-11'    # min.: 2008-06-02 (2-day and 5-day matrix) --- further: 2011-05-28|2014-01-02|2018-12-27enddate = '2009-12-31'      # max.: 2020-12-26 (last common day in 2-day and 5-day matrix) ---  2016-05-31|2020-01-01timewindow = 'all''''# What time-window would you like to use? (default: all -> specific_windows = False)specific_windows = Falseif specific_windows is False:    timewindow = 'all'else:    print('Please enter time window length of data to process: 01h  03h  06h  12h  01d  02d  05d')    timewindow = input()'''# FOR JC FEATURES:# Use JC matrix?Art_matrix = Falsemaxfreq = 5# Plot Spectrogram?plot_features = False# Plot SOM stuff?plot_SOM = False# SOM SETTINGS#mapsize:mx = [5,10,15,10,16,20,30]   #[6,6,20]   #    [5,6,6,10,10,16,20,20,30]  # dimensions of SOMmy = [5,10,15,40,25,20,30]   #  #[12,16,20]   #    [5,12,16,10,40,25,20,20,30]#map_lattice:map_lattice = ['rect','rect','rect','rect','rect','rect','rect']#   ['rect','hexa','hexa','rect','rect','rect','rect','hexa','rect']  # map shape#number of clusters:n_clusters = 5### PROCESSING MODES #### Are training and test data sets the same?TESTisTRAIN = Trueif TESTisTRAIN is False:    # Specify time period of test data (data you'd be looking at in the end):    startdate_test = '2011-06-01'  # min.: 2008-06-01 (5-day matrix)    enddate_test = '2014-06-01'  # max.: 2020-12-31 (last day in data)    # Do you already have a trained map?    trainedmap = True        # If so: Where can the trained map be found?    trained_PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/trained_maps/whakaari_43200.00wndw_rsam_10_2.00-5.00_data_features_5cl_5x5_2021-01-01_2021-11-21.pkl'else:    # Do you already have a trained map?    trainedmap = False# Shall the trained map be saved (or not, to save disk space)?save_trained_map = True# Shall the maps be tested and plotted or just trained?Test_Plot = Truemonthinterval = 1   #interval of x-ticks in months (adapt for visual reasons depending on test length)# Shall the maps be interactive for close-ups?interactive = False# Would you like to compute SOM errors?heatmap_on = True### CLUSTER NUMBER ANALYSIS #### Do you want to calculate a suggested number of cluster?CL_Determination = True# Shall the cluster curves be plotted? (if CL_Determination = True)plot_curves = True# Shall the cluster numbers be calculated using linear or polynomial detrending (of what degree)? (if CL_Determination = True)Polynomial_Detrend = Truedegree = 4      # default: 2# Shall an existing file containing the suggested number of clusters be continued? (if CL_Determination = True)keep_CL_mode = True########### TRAINING STAGE ###########if TESTisTRAIN is True:    #PATH = os.getcwd()+'/feature_matrices/{:s}/'.format(timewindow)    if Art_matrix is True:        PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/Art_matrices/new_matrices/ready/'        fl = PATH + '*.csv'        files = glob.glob(fl)        FILE = []        EXTNAME = []        fbands = [x[(len(PATH)+4):-6] for x in files]     # removes everything before and after the frequency band info in the file name    else:        #PATH = '/Users/bst/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/feature_matrices/'        PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/feature_matrices/'        fl = PATH + '*.csv'        p = len(PATH)        p2 = p+31                               # 31 symbols to remove from the file name to extract 'fband1-fband2'        files = glob.glob(fl)        FILE = []        EXTNAME = []        fbands = [x[p2:-18] for x in files]     # removes everything before and after the frequency band info in the file name    if CL_Determination is True:        cl_mode = []    for z in range (len(mx)):        mapsize = [mx[z], my[z]]        lattice = map_lattice[z]        Errors = []        names = []        for i in range(len(files)):            Graphics = False  # False disables graphics, e.g. for batch            logging.getLogger('matplotlib.font_manager').disabled = True            plt.interactive(False)            if Art_matrix is True:                FILE = files[i][92:-4]   # removes ".csv"                filename = PATH + FILE + '.csv'                df = pd.read_csv(filename, header=None, skiprows=0)                time_np = df[0]                diffstart = date(1, 1, 1)                diffend = date(1971, 1, 3)                secs = (diffend - diffstart).total_seconds()                for i in range(len(time_np)):                    if time_np[i] == 0:                        time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])                new_time = time_np * 24 * 60 * 60 - secs                time_np = []                for d in range(len(new_time)):                    if new_time[d] < 0:                        gap = 0                        time_np.append(gap)                    else:                        time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))            else:                FILE = files[i][p:-4]  # removes ".csv"                filename = PATH + FILE + '.csv'                df = pd.read_csv(filename, header=None, low_memory=False)                time_np = df[0]            dates = list(time_np)            if len(dates[1]) < 11:                newdates = dates                for row in newdates:                    if startdate == row:                        to_start = np.array(newdates.index(startdate))                        break                for row in newdates:                    if enddate == row:                        to_end = np.array(newdates.index(enddate))                        break                to_end = to_end - to_start            else:                newdates = [x[:-9] for x in dates]                for row in newdates:                    if startdate == row:                        to_start = np.array(newdates.index(startdate))                        break                for row in newdates:                    if enddate == row:                        to_end = np.array(newdates.index(enddate))                        break                to_end = to_end - to_start            df = pd.read_csv(filename, header=None, skiprows=(int(to_start)), nrows=(int(to_end)),low_memory=False)            dlen = df.shape[0]            df.head()            dfselection2=pd.DataFrame(data=df.iloc[0:dlen, 1:df.shape[1]])  # columns from 1 to 5. Not using column 0 which is time            Traindata = dfselection2.values            som = sompy.SOMFactory.build(Traindata, mapsize, mask=None, mapshape='planar', lattice=lattice, normalization='var', initialization='pca', neighborhood='gaussian', training='batch', name='sompy')  # this will use the default parameters, but i can change the initialization and neighborhood methods            #all_mapshapes = ['planar','toroid','cylinder'] ## Only planar is implemented!!!            #all_lattices = ['hexa','rect']            #all_initialization = ['random','pca']            som.train(n_job=6, verbose='debug')              #verbose='info' medium. verbose='debug' will print more, and verbose=None wont print anything            te = som.calculate_topographic_error()            qe = np.mean(som._bmu[1])            print ("Topographic error = %s; Quantization error = %s" % (te,qe))            teqe = (te,qe)            Errors.append(teqe)            EXTNAME = '_{:s}cl_{:s}x{:s}_{:s}_{:s}.pkl'.format(str(n_clusters),str(mx[z]),str(my[z]),startdate,enddate)            names.append(str(FILE+EXTNAME))            cl = som.cluster(n_clusters=n_clusters)            if plot_SOM is True:                v = sompy.mapview.View2DPacked(50, 50, 'test', text_size=8)                fig = v.show(som, what='codebook', which_dim='all', cmap='jet', col_sz=6)  # which_dim='all' default                plt.savefig('mapview.png', dpi=200)                v = sompy.mapview.View2DPacked(2, 2, 'test', text_size=8)                v.show(som, what='cluster')                h = sompy.hitmap.HitMapView(10, 10, 'hitmap', text_size=8, show_text=True)                h.show(som)                vhts = sompy.visualization.bmuhits.BmuHitsView(40,40,"Hits Map",text_size=12)                vhts.show(som, anotate=True, onlyzeros=False, labelsize=12, cmap="Greys", logaritmic=False)                plts = sompy.visualization.dotmap.DotMapView(40, 40, "Dot Map", text_size=12)                plts.show(som) #lentissimo                u = sompy.umatrix.UMatrixView(50, 50, 'umatrix', show_axis=True, text_size=8, show_text=True)                UMAT = u.build_u_matrix(som, distance=1, row_normalized=False)                UMAT = u.show(som, distance2=1, row_normalized=False, show_data=True, contooor=True, blob=False)            #if not os.path.isdir('trained_maps'):            # os.makedirs('trained_maps')            getattr(som, 'cluster_labels')            if save_trained_map is True:                #joblib.dump(som, '/Volumes/TheBigOne/Data/Maps/'+FILE+EXTNAME)                if Art_matrix is True:                    joblib.dump(som,'/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/trained_maps/' + FILE + EXTNAME)                else:                    if extdrive is True:                        joblib.dump(som, '/Volumes/TheBigOne/Data/Maps/2019 NO/trained_maps/' + EXTNAME)                    else:                        joblib.dump(som, '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/trained_maps/' + FILE + EXTNAME)            ########## CL DETERMINATION (OPTIONAL) ##########            mapname = FILE+EXTNAME[:-4]            if CL_Determination is True:                def cl_det(som, cl, mapname, plot_curves, degree):                    register_matplotlib_converters()  # to avoid warnings                    import logging                    from sklearn.mixture import GaussianMixture                    from itertools import combinations                    from sklearn.cluster import KMeans                    from sklearn.preprocessing import MinMaxScaler                    from sklearn.metrics import silhouette_score, davies_bouldin_score, v_measure_score                    graphics = False  # False disables graphics, e.g. for batch                    ppl.interactive(False)                    logging.getLogger('matplotlib.font_manager').disabled = True                    som_vectors = som.codebook.matrix                    Min_num_clusters = 2    # default: 2                    Max_num_clusters = 13   # default: 13 (12 the last no to be calculated)                    y = cl                    n_samples = som_vectors.shape[0]                    n_features = som_vectors.shape[1]                    print("number of samples", n_samples)                    print("number of features", n_features)                    d1 = som_vectors                    d1.shape                    df1 = pd.DataFrame(data=d1, columns=['Feature_' + str(i) for i in range(1, n_features + 1)])                    df1.head()                    lst_vars = list(combinations(df1.columns, 2))                    len(lst_vars)                    '''                    ### PLOTTING SECTION ###                        total_num_subplots = len(lst_vars)                        start_num_subplot = 1                    end_num_subplot = total_num_subplots                    # per Lipari sono troppi, limitare a max circa 100, va fatta la scelta                    start_num_subplot = 101                    end_num_subplot = 150                        plot_num_columns = 3                    real_number_of_suplots = (end_num_subplot - start_num_subplot + 1)                        plot_num_rows = int(real_number_of_suplots / plot_num_columns) + 1                    ppl.figure(figsize=(20, plot_num_rows * 6))                        for i in range(1, real_number_of_suplots):                        ppl.subplot(plot_num_rows, plot_num_columns, i)                        dim1 = lst_vars[start_num_subplot + i - 1][0]                        dim2 = lst_vars[start_num_subplot + i - 1][1]                        ppl.scatter(df1[dim1], df1[dim2], c=y, edgecolor='k', s=150)                            ppl.xlabel(f"{dim1}", fontsize=13)                        ppl.ylabel(f"{dim2}", fontsize=13)                        num_of_boxplots = len(df1.columns)                    boxplots_num_columns = 3                    boxplots_num_rows = int(num_of_boxplots / boxplots_num_columns) + 1                    ppl.figure(figsize=(20, plot_num_rows * 6))                    for i, c in enumerate(df1.columns):                        ppl.subplot(boxplots_num_rows, boxplots_num_columns, i + 1)                        sns.boxplot(y=df1[c], x=y)                        ppl.xticks(fontsize=15)                        ppl.yticks(fontsize=15)                        ppl.xlabel("Cluster", fontsize=15)                        ppl.ylabel(c, fontsize=15)                        ppl.show()                        ### END PLOTTING SECTION ###                    '''                    X = df1                    X.head()                    scaler = MinMaxScaler()                    X_scaled = scaler.fit_transform(X)                    ### Calculation stage ###                    km_silhouette = []                    #vmeasure_score = []                    km_score = []                    db_score = []                    all_cl_no = []                    for i in range(Min_num_clusters, Max_num_clusters):                        km = KMeans(n_clusters=i, random_state=0).fit(X_scaled)                        preds = km.predict(X_scaled)                        silhouette = silhouette_score(X_scaled, preds)                        km_silhouette.append(silhouette)                        db = davies_bouldin_score(X_scaled, preds)                        db_score.append(db)                        #v_measure = v_measure_score(y, preds)                        #vmeasure_score.append(v_measure)                        km_score.append(-km.score(X_scaled))                    if Polynomial_Detrend is False:                        # (removes linear trend from score_values and calculates local maxima (Sil&V-Meas.) / minima (DB))                        loc_min_kms = ss.argrelextrema(ss.detrend(np.asarray(km_silhouette)), np.greater)                        loc_min_db = ss.argrelextrema(ss.detrend(np.asarray(db_score)), np.less)                        loc_min_v_meas = ss.argrelextrema(ss.detrend(np.asarray(vmeasure_score)), np.greater)                    else:                        # (removes polynomial trend from values and calculates local max/min)                        X = [i for i in range(Min_num_clusters, Max_num_clusters)]                        X = np.reshape(X, (len(X), 1))                        y1 = km_silhouette                        y2 = db_score                        #y3 = vmeasure_score                        y3 = km_score                        pf = PolynomialFeatures(degree=degree)                        Xp = pf.fit_transform(X)                        md2 = LinearRegression()                        md3 = LinearRegression()                        md4 = LinearRegression()                        md2.fit(Xp, y1)                        md3.fit(Xp, y2)                        md4.fit(Xp, y3)                        trendp1 = md2.predict(Xp)                        trendp2 = md3.predict(Xp)                        trendp3 = md4.predict(Xp)                        detrpoly1 = [y1[i] - trendp1[i] for i in range(0, len(y1))]                        detrpoly2 = [y2[i] - trendp2[i] for i in range(0, len(y2))]                        detrpoly3 = [y3[i] - trendp3[i] for i in range(0, len(y3))]                        loc_max_kms = ss.argrelextrema(np.asarray(detrpoly1), np.greater)                        loc_min_db = ss.argrelextrema(np.asarray(detrpoly2), np.less)                        #loc_max_v_meas = ss.argrelextrema(np.array(detrpoly3), np.greater)                        loc_km_score = ss.argrelextrema(np.array(detrpoly3), np.greater)                    #LKM = list(loc_max_kms[0])                    #LDB = list(loc_min_db[0])                    #LVM = list(loc_max_v_meas[0])                    LKM = [(i+Min_num_clusters) for i in list(loc_max_kms[0]) if (i+Min_num_clusters) > 3 and (i+Min_num_clusters) < 11]                    LDB = [(i+Min_num_clusters) for i in list(loc_min_db[0]) if (i+Min_num_clusters) > 3 and (i+Min_num_clusters) < 11]                    #LVM = [(i+Min_num_clusters) for i in list(loc_max_v_meas[0]) if (i+Min_num_clusters) > 3 and (i+Min_num_clusters) < 11]                    KMS = [(i+Min_num_clusters) for i in list(loc_km_score[0]) if (i+Min_num_clusters) > 3 and (i+Min_num_clusters) < 11]                    for x1 in range(len(LKM)):                        all_cl_no.append(list(LKM)[x1])     # +MinNumCL to display number of cluster, not index in list                    for x2 in range(len(LDB)):                        all_cl_no.append(list(LDB)[x2])                    #for x3 in range(len(LVM)):                    #    all_cl_no.append(list(LVM)[x3])                    for x3 in range(len(KMS)):                        all_cl_no.append(list(KMS)[x3])                    #votes = []                    all_cl = list(multimode(all_cl_no))                    if len(all_cl)==1:                        for i in range(6):                            cl_mode.append(all_cl[0])                        #cl_mode.append(votes)                    elif len(all_cl)==2:                        for j in range(3):                            for i in range(len(all_cl)):                                cl_mode.append(all_cl[i-1])                        #cl_mode.append(votes)                    elif len(all_cl)==3:                        for j in range(2):                            for i in range(len(all_cl)):                                cl_mode.append(all_cl[i-1])                        #cl_mode.append(votes)                    else:                        all_cl = list(multimode(all_cl_no))[:3]                        for j in range(2):                            for i in range(len(all_cl)):                                cl_mode.append(all_cl[i-1])                        #cl_mode.append(votes)                    #for x4 in range(len(all_cl)):                    #    cl_mode.append(all_cl[x4])                    ### Plotting stage ###                    if plot_curves is True:                        # Min_num_clusters_for_plot=2                        # Max_num_clusters_for_plot=15                        Min_num_clusters_for_plot = Min_num_clusters                        Max_num_clusters_for_plot = Max_num_clusters                        circlesize = 60  # size of circles in the graphs                        hsize = 16  # size of each graph (horizontal)                        vsize = 5  # size of each graph (vertical)                        ### VISUAL OUTPUTS ###                        logging.getLogger('matplotlib.font_manager').disabled = True                        if not os.path.isdir('cl_OUTPUT'):                            os.makedirs('cl_OUTPUT')                        # Elbow method                        ppl.figure(figsize=(hsize, vsize))                        ppl.title("Elbow: "+mapname, fontsize=16)                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                    y=km_score[                                      Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                    s=circlesize, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                 km_score[                                 Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                 color='r')                        ppl.grid(True)                        ppl.xlabel("Number of clusters", fontsize=14)                        ppl.ylabel("K-means score", fontsize=15)                        ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                        ppl.yticks(fontsize=15)                        ppl.savefig('cl_OUTPUT/elbow_' + mapname + '.png', dpi=200)                            '''                        # Elbow method, Derivative                        derivative_km = np.diff(km_scores)                        ppl.figure(figsize=(hsize, vsize))                        ppl.title("The derivative of elbow method for determining number of clusters\n", fontsize=16)                        Max_num_clusters_for_plot_1 = Max_num_clusters_for_plot - 1                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot_1)], y=derivative_km[                                                                                                                    Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot_1 - Min_num_clusters],                                    s=circlesize, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot_1)],                                 derivative_km[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot_1 - Min_num_clusters],                                 color='r')                        ppl.grid(True)                        ppl.xlabel("Number of clusters", fontsize=14)                        ppl.ylabel("K-means score", fontsize=15)                        ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot_1)], fontsize=14)                        ppl.yticks(fontsize=15)                        ppl.savefig('cl_OUTPUT/elbow_derivative'+map+'.png', dpi=200)                                                # V-Measure for Evaluating Clustering Performance (compared to known labels)                        ppl.figure(figsize=(hsize, vsize))                        ppl.title("V-Measure: " + mapname, fontsize=16)                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                    y=vmeasure_score[                                      Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                    s=circlesize, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                 vmeasure_score[                                 Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                 color='r')                        ppl.grid(True)                        ppl.xlabel("Number of clusters", fontsize=14)                        ppl.ylabel("V-measure score", fontsize=15)                        ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                        ppl.yticks(fontsize=15)                        ppl.savefig('cl_OUTPUT/V-measure_' + mapname + '.png', dpi=200)                        '''                        # Silhouette method                        ppl.figure(figsize=(hsize, vsize))                        ppl.title("Silhouette: " + mapname, fontsize=16)                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                    y=km_silhouette[                                      Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                    s=circlesize, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                 km_silhouette[                                 Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                 color='r')                        ppl.grid(True)                        ppl.xlabel("Number of clusters", fontsize=14)                        ppl.ylabel("Silhouette score", fontsize=15)                        ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                        ppl.yticks(fontsize=15)                        ppl.savefig('cl_OUTPUT/silhouette_' + mapname + '.png', dpi=200)                        # Davies-Bouldin score                        ppl.figure(figsize=(hsize, vsize))                        ppl.title("D-B Score: " + mapname, fontsize=16)                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                    y=db_score[                                      Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                    s=circlesize, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                 db_score[                                 Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                 color='r')                        ppl.grid(True)                        ppl.xlabel("Number of clusters", fontsize=14)                        ppl.ylabel("Davies-Bouldin score", fontsize=15)                        ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                        ppl.yticks(fontsize=15)                        ppl.savefig('cl_OUTPUT/Davies-Bouldin_' + mapname + '.png', dpi=200)                        '''                        # BIC score with a Gaussian Mixture Model                        ppl.figure(figsize=(hsize, vsize))                        ppl.title("The Gaussian Mixture model BIC (red line) and AIC (blue line)", fontsize=16)                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                    y=np.log(                                        gm_bic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                    s=circlesize, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                 np.log(gm_bic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                 color='r')                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                    y=np.log(                                        gm_aic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                    s=circlesize, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                 np.log(gm_aic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                 color='b')                        ppl.grid(True)                        ppl.xlabel("Number of clusters", fontsize=14)                        ppl.ylabel("Log of Gaussian mixture BIC score", fontsize=15)                        ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                        ppl.yticks(fontsize=15)                        ppl.savefig('cl_OUTPUT/BIC(GMM)'+mapname+'.png', dpi=200)                                                    # (Log-)likelihood score                        ppl.figure(figsize=(hsize, vsize))                        ppl.title('Log-L-Score: '+ mapname, fontsize=16)                        ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                    y=gm_score[                                      Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                    s=150, edgecolor='k')                        ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                 gm_score[                                 Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                 color='r')                        ppl.grid(True)                        ppl.xlabel("Number of clusters", fontsize=14)                        ppl.ylabel("Gaussian mixture GM_score", fontsize=15)                        ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                        ppl.yticks(fontsize=15)                        ppl.savefig('cl_OUTPUT/(Log-)Likelihood_score_' + mapname + '.png', dpi=200)                        '''                cl_det(som,cl,mapname,plot_curves,degree)            ########## TESTING STAGE ##########            #'''            if Test_Plot is True:                output = som.project_data(Traindata)                outputDF = pd.DataFrame(output) #for each data, outputDF contains its assigned SOM node (bmu)                cl = som.cluster(n_clusters=n_clusters)                cloutput = cl[output] #for each data, cloutput contains its assigned cluster                cloutputDF = pd.DataFrame(cloutput)                cloutputDF.to_csv('Clusters.csv', index=False, header=False)                '''                if Art_matrix is True:                    time_np = df[0]                    diffstart = date(1, 1, 1)                    diffend = date(1971, 1, 3)                    secs = (diffend - diffstart).total_seconds()                    for i in range(len(time_np)):                        if time_np[i] == 0:                            time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])                    new_time = time_np * 24 * 60 * 60 - secs                    time_np = []                    for d in range(len(new_time)):                        if new_time[d] < 0:                            gap = 0                            time_np.append(gap)                        else:                            time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))                                else:                    time_np = df[0]                '''                to_start, to_end = int(to_start), int(to_end)                time_np = time_np[to_start:(to_start + to_end)]                if not os.path.isdir('tested_maps'):                 os.makedirs('tested_maps')                if Art_matrix is True:                    with open('tested_maps/'+FILE+EXTNAME, 'wb') as f:  # Python 3: open(..., 'wb')                     pickle.dump([time_np, cloutput], f)                else:                    if extdrive is True:                        with open('/Volumes/TheBigOne/Data/Maps/2019 NO/tested_maps/'+FILE+EXTNAME, 'wb') as f:  # Python 3: open(..., 'wb')                         pickle.dump([time_np, cloutput], f)                    else:                        with open('/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/tested_maps/' + FILE + EXTNAME, 'wb') as f:                         pickle.dump([time_np, cloutput], f)                ########### PLOTTING STAGE ###########                stationComp = 'WIZHHZ'  # just for the title                register_matplotlib_converters()  # to avoid warnings                if Art_matrix is True:                    with open('tested_maps/' + FILE+EXTNAME, 'rb') as f:  # Python 3: open(..., 'wb')                     time_np, cloutput = pickle.load(f)                else:                    if extdrive is True:                        with open('/Volumes/TheBigOne/Data/Maps/2019 NO/tested_maps/'+FILE+EXTNAME,'rb') as f:  # Python 3: open(..., 'wb')                         time_np, cloutput = pickle.load(f)                    else:                        with open('/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/tested_maps/' + FILE + EXTNAME,'rb') as f:                         time_np, cloutput = pickle.load(f)                time_np = np.asarray(time_np)                time_np2 = []                for x in range(len(time_np)):                 try:                  time_np2.append(datetime.strptime(str(time_np[x]), '%Y-%m-%d %H:%M:%S'))                 except:                  time_np2.append(datetime.strptime(str(time_np[x]), '%Y-%m-%d'))                time_np = np.array(time_np2)                # plot initialization                fig, axarr = plt.subplots(1, sharex=True, sharey=False)                if interactive is False:                    fig.set_size_inches(16, 9)                axarr.set_yticklabels([])                axarr.axis('off')                ax = fig.add_subplot(1, 1, 1, sharex=fig.axes[0])                colormap = np.array(['r', 'purple', 'b', 'cyan', 'lime'])                tminimo = 0                tmassimo = time_np.shape[0]                colours = cloutput[tminimo:tmassimo] % colormap.shape[0]                ax.scatter(time_np[tminimo:tmassimo], cloutput[tminimo:tmassimo]+1, c=colormap[colours])                ax.set_xlim(min(time_np), max(time_np))                ax.set_ylabel('Cluster number', fontsize = 15)                ax.grid(linestyle='dotted', which='both')                ax.yaxis.set_major_locator(MaxNLocator(integer=True))                #plt.pause(0.001)                fig.subplots_adjust(hspace=0.05)                plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)                # change interval of ticks on x-axis here depending on length of time-period covered by data                test_startdate = datetime.strptime('2010-12-30 00:00:00', '%Y-%m-%d %H:%M:%S')                if volcano=='whakaari':                    if min(time_np) < test_startdate:                        ax.xaxis.set_major_locator(mdates.YearLocator(base = 1, month = 1, day = 1))                    else:                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=monthinterval))                else:                    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=monthinterval))                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))                fig.autofmt_xdate()                plt.xticks(fontsize = 15)                plt.yticks(fontsize = 15)                # add events to plot                with open('act_log/eruptive_periods.txt', 'r') as fp:                    tes = [ln.rstrip() for ln in fp.readlines()]                xcoords = tes                for xc in xcoords:                    ax.axvline(x = xc, color='k', linestyle='-', linewidth=2, label='_')                with open('act_log/activity.txt', 'r') as fp:                    act = [ln.rstrip() for ln in fp.readlines()]                cords = act                for co in cords:                    ax.axvline(x = co, color='dimgrey', linestyle='--', linewidth=2, label='_')                ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2, label='ash emission')                ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2, label='observation of lava dome')                # add legend and title                #ax.legend(bbox_to_anchor=(0.25, 1), fontsize = 15, ncol = 3)                #TITLE = str(FILE+'   Cluster: ' + str(n_clusters) + '   SOM size: '+str(mx[z])+'x'+str(my[z]))                TITLE = str(volcano + ', ' + station + '   fband: ' + filename[108:-18] +'   Cluster: ' + str(n_clusters) + '   SOM size: '+str(mx[z])+'x'+str(my[z]))                ppl.title(TITLE, loc='left')                # save plot                fig.set_size_inches(16, 6)                if not os.path.isdir('../__OUTPUT'):                 os.makedirs('../__OUTPUT')                if Art_matrix is True:                    path = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/__OUTPUT/plots/'+ FILE + EXTNAME + '.png'                else:                    if extdrive is True:                        path = '/Volumes/TheBigOne/Data/Maps/2019 NO/plots/'+FILE+EXTNAME+'.png'                    else:                        path = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/__OUTPUT/plots/' + FILE + EXTNAME + '.png'                plt.savefig(path, dpi=600)                if interactive is True:                    fig.set_size_inches(15,8)                    ppl.ion()                    ppl.show()                    ppl.pause(1000)                plt.close()                if Art_matrix is True:                    if plot_features is True:                        # RSEM                        if not os.path.isdir('../__OUTPUT/plots/meanRSAM_plots'):                            os.makedirs('../__OUTPUT/plots/meanRSAM_plots')                        # df = pd.read_csv(filename, header=None, skiprows=0)                        meanRSAM = df[1]                        peakfreq = df[2]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, meanRSAM, '-', c='k', label='Energy')                        ax.set_xlabel('Time')                        ax.set_ylabel('RSEM')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                        plt.ylim(0, 0.00002)                        ppl.margins(x=0)                        ax.set_xlim(min(time_np), max(time_np))                        fig.autofmt_xdate()                        # add events to plot                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        path = '../__OUTPUT/plots/meanRSAM_plots/meanRSAM_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)                        # Dominant Frequency                        if not os.path.isdir('../__OUTPUT/plots/PeakFreq_plots'):                            os.makedirs('../__OUTPUT/plots/PeakFreq_plots')                        # df = pd.read_csv(filename, header=None, skiprows=0)                        meanRSAM = df[1]                        peakfreq = df[2]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, peakfreq, '-', c='r', label='Peak Frequency')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                        # add events to plot:                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        fig.autofmt_xdate()                        ax.set_xlabel('Time')                        ax.set_ylabel('peak frequency')                        plt.ylim(0, maxfreq)                        ax.set_xlim(min(time_np), max(time_np))                        ppl.margins(x=0)                        path = '../__OUTPUT/plots/PeakFreq_plots/PeakFreq_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)                        # Mean Frequency                        if not os.path.isdir('../__OUTPUT/plots/MeanFreq_plots'):                            os.makedirs('../__OUTPUT/plots/MeanFreq_plots')                        # df = pd.read_csv(filename, header=None, skiprows=0)                        meanfreq = df[3]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, meanfreq, '-', c='r', label='Mean Frequency')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                        # add events to plot                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        fig.autofmt_xdate()                        ax.set_xlim(min(time_np), max(time_np))                        ax.set_xlabel('Time')                        ax.set_ylabel('Mean frequency')                        plt.ylim(2, 5)                        ppl.margins(x=0)                        path = '../__OUTPUT/plots/MeanFreq_plots/MeanFreq_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)                        # Frequency Standard Deviation                        if not os.path.isdir('../__OUTPUT/plots/FreqSTD_plots'):                            os.makedirs('../__OUTPUT/plots/FreqSTD_plots')                            # df = pd.read_csv(filename, header=None, skiprows=0)                        freqstd = df[4]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, freqstd, '-', c='r', label='Frequency STD')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                        # add events to plot                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        fig.autofmt_xdate()                        ax.set_xlim(min(time_np), max(time_np))                        ax.set_xlabel('Time')                        ax.set_ylabel('Frequency standard deviation')                        plt.ylim(0.5, 1)                        ppl.margins(x=0)                        path = '../__OUTPUT/plots/FreqSTD_plots/FreqSTD_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)        ########### HEATMAP ###########        if heatmap_on is True:            # generate csv containing TE and QE for each map            rownames = ['te', 'qe']            TE_QE = pd.DataFrame(Errors, columns=rownames, index=names)            TE_QE = TE_QE.rename_axis('map_names').sort_values(by='map_names')            TE_QE.to_csv("SOM_accuracy.csv")            df = pd.read_csv('/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/SOM_accuracy.csv',                             header=None, skiprows=1)            SOMsize = []            for r in range(len(mx)):                SOMsize.append([mx[r], my[r]])            fbands = sorted(fbands)            TEnorm = df[1]            QEnorm = df[2]            fbands = ['0.1-1.0','1.0-2.0','1.0-15.0','2.0-5.0','5.0-10.0','6.0-8.0','10.0-15.0']            w_len = ['1 h','3 h','6 h','12 h','1 d','2 d','5 d']            mapsize_heat = str(mx[z]) + ' x ' + str(my[z])            # SOMsize = ['5 x 5 (r)','6 x 12 (h)','6 x 16 (h)','10 x 10 (r)','10 x 40 (h)','16 x 25 (r)','20 x 20 (r)','20 x 20 (h)','30 x 30 (r)']            # Topographic error            cont = np.asarray(TEnorm).reshape(len(w_len), len(fbands))            norm = LogNorm(vmin=0.000001, vmax=1)            title = 'Topographic Error      Cluster: ' + str(n_clusters) + '      Mapsize: ' + mapsize_heat            sns.set(font_scale=1.2)            fig, ax = plt.subplots(figsize=(20, 8))            # ticks = [0.5,1]            cmap = sns.color_palette("cool", as_cmap=True)            ax = sns.heatmap(cont, linewidth=0.5, norm=norm, cmap=cmap, cbar=True, xticklabels=fbands,                             yticklabels=w_len, cbar_kws={'label': 'Topographic Error'})            ax.set_ylabel('Window length', fontsize=20)            ax.set_xlabel('Frequency band [Hz]', fontsize=20)            plt.title(title, fontsize=25)            if not os.path.isdir('../__OUTPUT/heatmapsTE'):                os.makedirs('../__OUTPUT/heatmapsTE')            plt.savefig('../__OUTPUT/heatmapsTE/' + 'TE ' + volcano + ' ' + station + ' ' + mapsize_heat + '.png',                        dpi=300)            # Quantization error            cont = np.asarray(QEnorm).reshape(len(w_len), len(fbands))  # reshape(SOMsize,fbands)            title = 'Quantisation Error      Cluster: ' + str(n_clusters) + '      Mapsize: ' + mapsize_heat            sns.set(font_scale=1.2)            fig, ax = plt.subplots(figsize=(20, 6))            colours3 = sns.color_palette("cool", as_cmap=True)            ax = sns.heatmap(cont, linewidth=0.5, cmap=colours3, cbar=True, vmax=20, vmin=0, xticklabels=fbands,                             yticklabels=w_len, cbar_kws={'label': 'Quantisation Error'})            ax.set_ylabel('Window length', fontsize=20)            ax.set_xlabel('Frequency band [Hz]', fontsize=20)            plt.title(title, fontsize=25)            if not os.path.isdir('../__OUTPUT/heatmapsQE'):                os.makedirs('../__OUTPUT/heatmapsQE')            plt.savefig('../__OUTPUT/heatmapsQE/' + 'QE ' + volcano + ' ' + station + ' ' + mapsize_heat + '.png',                        dpi=300)else:    if trainedmap is True:        Graphics = False  # False disables graphics, e.g. for batch        logging.getLogger('matplotlib.font_manager').disabled = True        plt.interactive(False)        mapsize = [5,5]        lattice = ['rect']        Errors = []        names = []        EXTNAME = '_{:s}cl_{:s}x{:s}_{:s}_{:s}.pkl'.format(str(n_clusters), str(mx[0]), str(my[0]), startdate_test, enddate_test)        # GET TEST DATA        if Art_matrix is True:            PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/Art_matrices/new_matrices/ready/'            fl = PATH + '*.csv'            files = glob.glob(fl)            FILE = []            fbands = [x[(len(PATH)+4):-6] for x in files]            FILE = files[0][92:-4]  # removes ".csv"            filename = PATH + FILE + '.csv'            df = pd.read_csv(filename, header=None, skiprows=0)            time_np = df[0]            # convert time scale            diffstart = date(1, 1, 1)            diffend = date(1971, 1, 3)            secs = (diffend - diffstart).total_seconds()            for i in range(len(time_np)):                if time_np[i] == 0:                    time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])            new_time = time_np * 24 * 60 * 60 - secs            time_np = []            for d in range(len(new_time)):                if new_time[d] < 0:                    gap = 0                    time_np.append(gap)                else:                    time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))            dates = list(time_np)            newdates = [x[:-9] for x in dates]            for row in newdates:                if startdate_test == row:                    to_start = np.array(newdates.index(startdate_test))                    break            for row in newdates:                if enddate_test == row:                    to_end = np.array(newdates.index(enddate_test))                    break            to_end = to_end - to_start            df = pd.read_csv(filename, header=None, skiprows=(int(to_start)), nrows=(int(to_end)))            time_np = df[0]            # convert time scale            diffstart = date(1, 1, 1)            diffend = date(1971, 1, 3)            secs = (diffend - diffstart).total_seconds()            for i in range(len(time_np)):                if time_np[i] == 0:                    time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])            new_time = time_np * 24 * 60 * 60 - secs            time_np = []            for d in range(len(new_time)):                if new_time[d] < 0:                    gap = 0                    time_np.append(gap)                else:                    time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))            dates = list(time_np)        else:            PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/feature_matrices/'            fl = PATH + '*.csv'            p = len(PATH)            p2 = p + 31  # 31 symbols to remove from the file name to extract 'fband1-fband2'            files = glob.glob(fl)            FILE = []            fbands = [x[p2:-18] for x in files]  # removes everything before and after the frequency band info in the file name            FILE = files[0][p:-4]  # removes ".csv"            filename = PATH + FILE + '.csv'            df = pd.read_csv(filename, header=None)            time_np = df[0]            dates = list(time_np)            newdates = [x[:-9] for x in dates]            for row in newdates:                if startdate_test == row:                    to_start = np.array(newdates.index(startdate_test))                    break            for row in newdates:                if enddate_test == row:                    to_end = np.array(newdates.index(enddate_test))                    break            to_end = to_end - to_start            df = pd.read_csv(filename, header=None, skiprows=(int(to_start)), nrows=(int(to_end)))        dlen = df.shape[0]        time_np = df[0]        dfselection2 = pd.DataFrame(            data=df.iloc[0:dlen, 1:df.shape[1]])        Testdata = dfselection2.values        # LOAD TRAINED MAP        ### Insert trained map here:        som = joblib.load(trained_PATH)        # TEST DATA ON TRAINED MAP        output = som.project_data(Testdata)        outputDF = pd.DataFrame(output)  # for each data, outputDF contains its assigned SOM node (bmu)        cl = som.cluster(n_clusters=n_clusters)        cloutput = cl[output]  # for each data, cloutput contains its assigned cluster        cloutputDF = pd.DataFrame(cloutput)        cloutputDF.to_csv('Clusters.csv', index=False, header=False)        # SAVE MAP        if not os.path.isdir('tested_maps'):            os.makedirs('tested_maps')        if Art_matrix is True:            with open('tested_maps/' + FILE + EXTNAME, 'wb') as f:  # Python 3: open(..., 'wb')                pickle.dump([time_np, cloutput], f)        else:            if extdrive is True:                with open('/Volumes/TheBigOne/Data/Maps/2019 NO/tested_maps/' + FILE + EXTNAME, 'wb') as f:  # Python 3: open(..., 'wb')                    pickle.dump([time_np, cloutput], f)            else:                with open('/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/tested_maps/' + FILE + EXTNAME, 'wb') as f:                    pickle.dump([time_np, cloutput], f)        ########### PLOTTING STAGE ###########        stationComp = 'WIZHHZ'  # just for the title        register_matplotlib_converters()  # to avoid warnings        if Art_matrix is True:            with open('tested_maps/' + FILE + EXTNAME, 'rb') as f:  # Python 3: open(..., 'wb')                time_np, cloutput = pickle.load(f)        else:            if extdrive is True:                with open('/Volumes/TheBigOne/Data/Maps/2019 NO/tested_maps/' + FILE + EXTNAME,                          'rb') as f:  # Python 3: open(..., 'wb')                    time_np, cloutput = pickle.load(f)            else:                with open('/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/tested_maps/' + FILE + EXTNAME,                          'rb') as f:                    time_np, cloutput = pickle.load(f)        time_np2 = []        for x in range(len(time_np)):            try:                time_np2.append(datetime.strptime(str(time_np[x]), '%Y-%m-%d %H:%M:%S'))            except:                time_np2.append(datetime.strptime(str(time_np[x]), '%Y-%m-%d'))        time_np = np.array(time_np2)        # plot initialization        fig, axarr = plt.subplots(1, sharex=True, sharey=False)        if interactive is False:            fig.set_size_inches(16, 9)        axarr.set_yticklabels([])        axarr.axis('off')        ax = fig.add_subplot(1, 1, 1, sharex=fig.axes[0])        colormap = np.array(['r', 'purple', 'b', 'cyan', 'lime'])        tminimo = 0        tmassimo = time_np.shape[0]        colours = cloutput[tminimo:tmassimo] % colormap.shape[0]        ax.scatter(time_np[tminimo:tmassimo], cloutput[tminimo:tmassimo] + 1, c=colormap[colours])        ax.set_xlim(min(time_np), max(time_np))        ax.set_ylabel('Cluster number')        ax.grid(linestyle='dotted', which='both')        ax.yaxis.set_major_locator(MaxNLocator(integer=True))        # plt.pause(0.001)        fig.subplots_adjust(hspace=0.05)        plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)        # change interval of ticks on x-axis here depending on length of time-period covered by data        test_startdate = datetime.strptime('2010-12-30 00:00:00', '%Y-%m-%d %H:%M:%S')        if min(time_np) < test_startdate:            ax.xaxis.set_major_locator(mdates.YearLocator(base=1, month=1, day=1, tz=None))        else:            ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))        fig.autofmt_xdate()        # add events to plot        with open('act_log/eruptive_periods.txt', 'r') as fp:            tes = [ln.rstrip() for ln in fp.readlines()]        xcoords = tes        for xc in xcoords:            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')        with open('act_log/activity.txt', 'r') as fp:            act = [ln.rstrip() for ln in fp.readlines()]        cords = act        for co in cords:            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2, label='ash emission')        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2, label='observation of lava dome')        # add legend and title        ax.legend(bbox_to_anchor=(0.79, 1))        TITLE = str(FILE + '   Cluster: ' + str(n_clusters) + '   SOM size: ' + str(mx[0]) + 'x' + str(my[0]))        ppl.title(TITLE, loc='left')        # save plot        fig.set_size_inches(16, 9)        if not os.path.isdir('../__OUTPUT'):            os.makedirs('../__OUTPUT')        if Art_matrix is True:            path = '../__OUTPUT/' + FILE + EXTNAME + '.png'        else:            if extdrive is True:                path = '/Volumes/TheBigOne/Data/Maps/2019 NO/plots/' + FILE + EXTNAME + '.png'            else:                path = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/__OUTPUT/plots/' + FILE + EXTNAME + '.png'        plt.savefig(path, dpi=200)        if interactive is True:            fig.set_size_inches(15, 8)            ppl.ion()            ppl.show()            ppl.pause(1000)        plt.close()        if Art_matrix is True:            if plot_features is True:                # RSEM                if not os.path.isdir('../__OUTPUT/plots/meanRSAM_plots'):                    os.makedirs('../__OUTPUT/plots/meanRSAM_plots')                # df = pd.read_csv(filename, header=None, skiprows=0)                meanRSAM = df[1]                peakfreq = df[2]                fig, ax = ppl.subplots(sharex=False, sharey=False)                fig.set_size_inches(15, 5)                ax.plot(time_np, meanRSAM, '-', c='k', label='Energy')                ax.set_xlabel('Time')                ax.set_ylabel('RSEM')                ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                plt.ylim(0, 0.00002)                ppl.margins(x=0)                ax.set_xlim(min(time_np), max(time_np))                fig.autofmt_xdate()                # add events to plot                with open('act_log/eruptive_periods.txt', 'r') as fp:                    tes = [ln.rstrip() for ln in fp.readlines()]                xcoords = tes                for xc in xcoords:                    ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                with open('act_log/activity.txt', 'r') as fp:                    act = [ln.rstrip() for ln in fp.readlines()]                cords = act                for co in cords:                    ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                           label='ash emission')                ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                           label='observation of lava dome')                path = '../__OUTPUT/plots/meanRSAM_plots/meanRSAM_' + FILE + EXTNAME + '.png'                plt.savefig(path, dpi=200)                # Dominant Frequency                if not os.path.isdir('../__OUTPUT/plots/PeakFreq_plots'):                    os.makedirs('../__OUTPUT/plots/PeakFreq_plots')                # df = pd.read_csv(filename, header=None, skiprows=0)                meanRSAM = df[1]                peakfreq = df[2]                fig, ax = ppl.subplots(sharex=False, sharey=False)                fig.set_size_inches(15, 5)                ax.plot(time_np, peakfreq, '-', c='r', label='Peak Frequency')                ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                # add events to plot:                with open('act_log/eruptive_periods.txt', 'r') as fp:                    tes = [ln.rstrip() for ln in fp.readlines()]                xcoords = tes                for xc in xcoords:                    ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                with open('act_log/activity.txt', 'r') as fp:                    act = [ln.rstrip() for ln in fp.readlines()]                cords = act                for co in cords:                    ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                           label='ash emission')                ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                           label='observation of lava dome')                fig.autofmt_xdate()                ax.set_xlabel('Time')                ax.set_ylabel('peak frequency')                plt.ylim(0, maxfreq)                ax.set_xlim(min(time_np), max(time_np))                ppl.margins(x=0)                path = '../__OUTPUT/plots/PeakFreq_plots/PeakFreq_' + FILE + EXTNAME + '.png'                plt.savefig(path, dpi=200)                # Mean Frequency                if not os.path.isdir('../__OUTPUT/plots/MeanFreq_plots'):                    os.makedirs('../__OUTPUT/plots/MeanFreq_plots')                # df = pd.read_csv(filename, header=None, skiprows=0)                meanfreq = df[3]                fig, ax = ppl.subplots(sharex=False, sharey=False)                fig.set_size_inches(15, 5)                ax.plot(time_np, meanfreq, '-', c='r', label='Mean Frequency')                ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                # add events to plot                with open('act_log/eruptive_periods.txt', 'r') as fp:                    tes = [ln.rstrip() for ln in fp.readlines()]                xcoords = tes                for xc in xcoords:                    ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                with open('act_log/activity.txt', 'r') as fp:                    act = [ln.rstrip() for ln in fp.readlines()]                cords = act                for co in cords:                    ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                           label='ash emission')                ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                           label='observation of lava dome')                fig.autofmt_xdate()                ax.set_xlim(min(time_np), max(time_np))                ax.set_xlabel('Time')                ax.set_ylabel('Mean frequency')                plt.ylim(2, 5)                ppl.margins(x=0)                path = '../__OUTPUT/plots/MeanFreq_plots/MeanFreq_' + FILE + EXTNAME + '.png'                plt.savefig(path, dpi=200)                # Frequency Standard Deviation                if not os.path.isdir('../__OUTPUT/plots/FreqSTD_plots'):                    os.makedirs('../__OUTPUT/plots/FreqSTD_plots')                    # df = pd.read_csv(filename, header=None, skiprows=0)                freqstd = df[4]                fig, ax = ppl.subplots(sharex=False, sharey=False)                fig.set_size_inches(15, 5)                ax.plot(time_np, freqstd, '-', c='r', label='Frequency STD')                ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                # add events to plot                with open('act_log/eruptive_periods.txt', 'r') as fp:                    tes = [ln.rstrip() for ln in fp.readlines()]                xcoords = tes                for xc in xcoords:                    ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                with open('act_log/activity.txt', 'r') as fp:                    act = [ln.rstrip() for ln in fp.readlines()]                cords = act                for co in cords:                    ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                           label='ash emission')                ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                           label='observation of lava dome')                fig.autofmt_xdate()                ax.set_xlim(min(time_np), max(time_np))                ax.set_xlabel('Time')                ax.set_ylabel('Frequency standard deviation')                plt.ylim(0.5, 1)                ppl.margins(x=0)                path = '../__OUTPUT/plots/FreqSTD_plots/FreqSTD_' + FILE + EXTNAME + '.png'                plt.savefig(path, dpi=200)    else:        Errors = []        names = []        # PATH = os.getcwd()+'/feature_matrices/{:s}/'.format(timewindow)        if Art_matrix is True:            PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/Art_matrices/new_matrices/ready/'            fl = PATH + '*.csv'            files = glob.glob(fl)            FILE = []            EXTNAME = []            fbands = [x[(len(PATH) + 4):-6] for x in files]        else:            PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/feature_matrices/'            fl = PATH + '*.csv'            p = len(PATH)            p2 = p + 31  # 31 symbols to remove from the file name to extract 'fband1-fband2'            files = glob.glob(fl)            FILE = []            EXTNAME = []            fbands = [x[p2:-18] for x in                      files]  # removes everything before and after the frequency band info in the file name        if CL_Determination is True:            cl_mode = []        for z in range(len(mx)):            mapsize = [mx[z], my[z]]            lattice = map_lattice[z]            for i in range(len(files)):                # TRAIN MAP WITH TRAIN DATA RANGE                Graphics = False  # False disables graphics, e.g. for batch                logging.getLogger('matplotlib.font_manager').disabled = True                plt.interactive(False)                if Art_matrix is True:                    FILE = files[i][92:-4]  # removes ".csv"                    print(FILE)                    filename = PATH + FILE + '.csv'                    df = pd.read_csv(filename, header=None, skiprows=0)                    time_np = df[0]                    diffstart = date(1, 1, 1)                    diffend = date(1971, 1, 3)                    secs = (diffend - diffstart).total_seconds()                    for i in range(len(time_np)):                        if time_np[i] == 0:                            time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])                    new_time = time_np * 24 * 60 * 60 - secs                    time_np = []                    for d in range(len(new_time)):                        if new_time[d] < 0:                            gap = 0                            time_np.append(gap)                        else:                            time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))                else:                    FILE = files[i][77:-4]  # removes ".csv"                    filename = PATH + FILE + '.csv'                    df = pd.read_csv(filename, header=None, skiprows=1)                    time_np = df[0]                dates = list(time_np)                print(startdate, enddate)                if len(dates[1]) < 11:                    newdates = dates                    for row in newdates:                        if startdate == row:                            to_start = np.array(newdates.index(startdate))                            break                    for row in newdates:                        if enddate == row:                            to_end = np.array(newdates.index(enddate))                            break                    to_end = to_end - to_start                else:                    newdates = [x[:-9] for x in dates]                    for row in newdates:                        if startdate == row:                            to_start = np.array(newdates.index(startdate))                            break                    for row in newdates:                        if enddate == row:                            to_end = np.array(newdates.index(enddate))                            break                    to_end = to_end - to_start                df = pd.read_csv(filename, header=None, skiprows=(int(to_start)), nrows=(int(to_end)), low_memory=False)                dlen = df.shape[0]                df.head()                dfselection2 = pd.DataFrame(                    data=df.iloc[0:dlen, 1:df.shape[1]])  # columns from 1 to 5. Not using column 0 which is time                Traindata = dfselection2.values                som = sompy.SOMFactory.build(Traindata, mapsize, mask=None, mapshape='planar', lattice=lattice,                                             normalization='var', initialization='pca', neighborhood='gaussian',                                             training='batch',                                             name='sompy')  # this will use the default parameters, but i can change the initialization and neighborhood methods                # all_mapshapes = ['planar','toroid','cylinder'] ## Only planar is implemented!!!                # all_lattices = ['hexa','rect']                # all_initialization = ['random','pca']                som.train(n_job=6, verbose='debug')                # verbose='info' medium. verbose='debug' will print more, and verbose=None wont print anything                te = som.calculate_topographic_error()                qe = np.mean(som._bmu[1])                print ("Topographic error = %s; Quantization error = %s" % (te, qe))                teqe = (te, qe)                Errors.append(teqe)                EXTNAME = '_{:s}cl_{:s}x{:s}_{:s}_{:s}.pkl'.format(str(n_clusters), str(mx[z]), str(my[z]), startdate,                                                                   enddate)                names.append(str(FILE + EXTNAME))                cl = som.cluster(n_clusters=n_clusters)                if plot_SOM is True:                    #v = sompy.mapview.View2DPacked(50, 50, 'test', text_size=8)                    #v.show(som, what='codebook', which_dim='all', cmap='jet', col_sz=6)  # which_dim='all' default                    #v = sompy.mapview.View2DPacked(2, 2, 'test', text_size=8)                    #v.show(som, what='cluster')                    h = sompy.hitmap.HitMapView(10, 10, 'hitmap', text_size=8, show_text=True)                    h.show(som)                    #vhts = sompy.visualization.bmuhits.BmuHitsView(40, 40, "Hits Map", text_size=12)                    #vhts.show(som, anotate=True, onlyzeros=False, labelsize=12, cmap="Greys", logaritmic=False)                    #plts = sompy.visualization.dotmap.DotMapView(40, 40, "Dot Map", text_size=12)                    #plts.show(som)  # lentissimo                    u = sompy.umatrix.UMatrixView(50, 50, 'umatrix', show_axis=True, text_size=8, show_text=True)                    UMAT = u.build_u_matrix(som, distance=1, row_normalized=False)                    UMAT = u.show(som, distance2=1, row_normalized=False, show_data=True, contooor=True, blob=False)                # if not os.path.isdir('trained_maps'):                # os.makedirs('trained_maps')                getattr(som, 'cluster_labels')                if save_trained_map is True:                    # joblib.dump(som, '/Volumes/TheBigOne/Data/Maps/'+FILE+EXTNAME)                    if Art_matrix is True:                        joblib.dump(som,                                    '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/trained_maps/' + FILE + EXTNAME)                    else:                        if extdrive is True:                            joblib.dump(som, '/Volumes/TheBigOne/Data/Maps/2019 NO/trained_maps/' + FILE + EXTNAME)                        else:                            joblib.dump(som,                                        '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/trained_maps/' + FILE + EXTNAME)                ########## CL DETERMINATION (OPTIONAL) ##########                mapname = FILE + EXTNAME[:-4]                if CL_Determination is True:                    def cl_det(som, cl, mapname, plot_curves, degree):                        register_matplotlib_converters()  # to avoid warnings                        import logging                        from sklearn.mixture import GaussianMixture                        Graphics = False  # False disables graphics, e.g. for batch                        ppl.interactive(False)                        logging.getLogger('matplotlib.font_manager').disabled = True                        som_vectors = som.codebook.matrix                        print(som_vectors)                        Min_num_clusters = 2  # default: 2                        Max_num_clusters = 13  # default: 13 (12 the last no to be calculated)                        y = cl                        n_samples = som_vectors.shape[0]                        n_features = som_vectors.shape[1]                        print("number of samples", n_samples)                        print("number of features", n_features)                        d1 = som_vectors                        d1.shape                        df1 = pd.DataFrame(data=d1, columns=['Feature_' + str(i) for i in range(1, n_features + 1)])                        df1.head()                        from itertools import combinations                        lst_vars = list(combinations(df1.columns, 2))                        len(lst_vars)                        '''                        ### PLOTTING SECTION ###                        total_num_subplots = len(lst_vars)                        start_num_subplot = 1                        end_num_subplot = total_num_subplots                        # per Lipari sono troppi, limitare a max circa 100, va fatta la scelta                        start_num_subplot = 101                        end_num_subplot = 150                        plot_num_columns = 3                        real_number_of_suplots = (end_num_subplot - start_num_subplot + 1)                        plot_num_rows = int(real_number_of_suplots / plot_num_columns) + 1                        ppl.figure(figsize=(20, plot_num_rows * 6))                        for i in range(1, real_number_of_suplots):                            ppl.subplot(plot_num_rows, plot_num_columns, i)                            dim1 = lst_vars[start_num_subplot + i - 1][0]                            dim2 = lst_vars[start_num_subplot + i - 1][1]                            ppl.scatter(df1[dim1], df1[dim2], c=y, edgecolor='k', s=150)                            ppl.xlabel(f"{dim1}", fontsize=13)                            ppl.ylabel(f"{dim2}", fontsize=13)                        num_of_boxplots = len(df1.columns)                        boxplots_num_columns = 3                        boxplots_num_rows = int(num_of_boxplots / boxplots_num_columns) + 1                        ppl.figure(figsize=(20, plot_num_rows * 6))                        for i, c in enumerate(df1.columns):                            ppl.subplot(boxplots_num_rows, boxplots_num_columns, i + 1)                            sns.boxplot(y=df1[c], x=y)                            ppl.xticks(fontsize=15)                            ppl.yticks(fontsize=15)                            ppl.xlabel("Cluster", fontsize=15)                            ppl.ylabel(c, fontsize=15)                            ppl.show()                        ### END PLOTTING SECTION ###                        '''                        from sklearn.cluster import KMeans                        X = df1                        X.head()                        from sklearn.preprocessing import MinMaxScaler                        scaler = MinMaxScaler()                        X_scaled = scaler.fit_transform(X)                        from sklearn.metrics import silhouette_score, davies_bouldin_score, v_measure_score                        ### Calculation stage ###                        km_silhouette = []                        vmeasure_score = []                        db_score = []                        all_cl_no = []                        for i in range(Min_num_clusters, Max_num_clusters):                            km = KMeans(n_clusters=i, random_state=0).fit(X_scaled)                            preds = km.predict(X_scaled)                            silhouette = silhouette_score(X_scaled, preds)                            km_silhouette.append(silhouette)                            db = davies_bouldin_score(X_scaled, preds)                            db_score.append(db)                            v_measure = v_measure_score(y, preds)                            vmeasure_score.append(v_measure)                        if Polynomial_Detrend is False:                            # (removes linear trend from score_values and calculates local maxima (Sil&V-Meas.) / minima (DB))                            loc_min_kms = ss.argrelextrema(ss.detrend(np.asarray(km_silhouette)), np.greater)                            loc_min_db = ss.argrelextrema(ss.detrend(np.asarray(db_score)), np.less)                            loc_min_v_meas = ss.argrelextrema(ss.detrend(np.asarray(vmeasure_score)), np.greater)                        else:                            # (removes polynomial trend from values and calculates local max/min)                            X = [i for i in range(Min_num_clusters, Max_num_clusters)]                            X = np.reshape(X, (len(X), 1))                            y1 = km_silhouette                            y2 = db_score                            y3 = vmeasure_score                            pf = PolynomialFeatures(degree=degree)                            Xp = pf.fit_transform(X)                            md2 = LinearRegression()                            md3 = LinearRegression()                            md4 = LinearRegression()                            md2.fit(Xp, y1)                            md3.fit(Xp, y2)                            md4.fit(Xp, y3)                            trendp1 = md2.predict(Xp)                            trendp2 = md3.predict(Xp)                            trendp3 = md4.predict(Xp)                            detrpoly1 = [y1[i] - trendp1[i] for i in range(0, len(y1))]                            detrpoly2 = [y2[i] - trendp2[i] for i in range(0, len(y2))]                            detrpoly3 = [y3[i] - trendp3[i] for i in range(0, len(y3))]                            loc_max_kms = ss.argrelextrema(np.asarray(detrpoly1), np.greater)                            loc_min_db = ss.argrelextrema(np.asarray(detrpoly2), np.less)                            loc_max_v_meas = ss.argrelextrema(np.array(detrpoly3), np.greater)                        # LKM = list(loc_max_kms[0])                        # LDB = list(loc_min_db[0])                        # LVM = list(loc_max_v_meas[0])                        LKM = [(i + Min_num_clusters) for i in list(loc_max_kms[0]) if (i + Min_num_clusters) > 3]                        LDB = [(i + Min_num_clusters) for i in list(loc_min_db[0]) if (i + Min_num_clusters) > 3]                        LVM = [(i + Min_num_clusters) for i in list(loc_max_v_meas[0]) if (i + Min_num_clusters) > 3]                        for x1 in range(len(LKM)):                            all_cl_no.append(list(LKM)[x1])  # +MinNumCL to display number of cluster, not index in list                        for x2 in range(len(LDB)):                            all_cl_no.append(list(LDB)[x2])                        for x3 in range(len(LVM)):                            all_cl_no.append(list(LVM)[x3])                        # votes = []                        all_cl = list(multimode(all_cl_no))                        if len(all_cl) == 1:                            for i in range(6):                                cl_mode.append(all_cl[0])                            # cl_mode.append(votes)                        elif len(all_cl) == 2:                            for j in range(3):                                for i in range(len(all_cl)):                                    cl_mode.append(all_cl[i - 1])                            # cl_mode.append(votes)                        elif len(all_cl) == 3:                            for j in range(2):                                for i in range(len(all_cl)):                                    cl_mode.append(all_cl[i - 1])                            # cl_mode.append(votes)                        else:                            all_cl = list(multimode(all_cl_no))[:3]                            for j in range(2):                                for i in range(len(all_cl)):                                    cl_mode.append(all_cl[i - 1])                            # cl_mode.append(votes)                        # for x4 in range(len(all_cl)):                        #    cl_mode.append(all_cl[x4])                        ### Plotting stage ###                        if plot_curves is True:                            # Min_num_clusters_for_plot=2                            # Max_num_clusters_for_plot=15                            Min_num_clusters_for_plot = Min_num_clusters                            Max_num_clusters_for_plot = Max_num_clusters                            circlesize = 60  # size of circles in the graphs                            hsize = 16  # size of each graph (horizontal)                            vsize = 5  # size of each graph (vertical)                            ### VISUAL OUTPUTS ###                            logging.getLogger('matplotlib.font_manager').disabled = True                            if not os.path.isdir('cl_OUTPUT'):                                os.makedirs('cl_OUTPUT')                            '''                            # Elbow method                            ppl.figure(figsize=(hsize, vsize))                            ppl.title("Elbow: "+mapname, fontsize=16)                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                        y=km_scores[                                          Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                        s=circlesize, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                     km_scores[                                     Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                     color='r')                            ppl.grid(True)                            ppl.xlabel("Number of clusters", fontsize=14)                            ppl.ylabel("K-means score", fontsize=15)                            ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                            ppl.yticks(fontsize=15)                            ppl.savefig('cl_OUTPUT/elbow_' + mapname + '.png', dpi=200)                            # Elbow method, Derivative                            derivative_km = np.diff(km_scores)                            ppl.figure(figsize=(hsize, vsize))                            ppl.title("The derivative of elbow method for determining number of clusters\n", fontsize=16)                            Max_num_clusters_for_plot_1 = Max_num_clusters_for_plot - 1                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot_1)], y=derivative_km[                                                                                                                        Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot_1 - Min_num_clusters],                                        s=circlesize, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot_1)],                                     derivative_km[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot_1 - Min_num_clusters],                                     color='r')                            ppl.grid(True)                            ppl.xlabel("Number of clusters", fontsize=14)                            ppl.ylabel("K-means score", fontsize=15)                            ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot_1)], fontsize=14)                            ppl.yticks(fontsize=15)                            ppl.savefig('cl_OUTPUT/elbow_derivative'+map+'.png', dpi=200)                            '''                            # V-Measure for Evaluating Clustering Performance (compared to known labels)                            ppl.figure(figsize=(hsize, vsize))                            ppl.title("V-Measure: " + mapname, fontsize=16)                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                        y=vmeasure_score[                                          Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                        s=circlesize, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                     vmeasure_score[                                     Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                     color='r')                            ppl.grid(True)                            ppl.xlabel("Number of clusters", fontsize=14)                            ppl.ylabel("V-measure score", fontsize=15)                            ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                       fontsize=14)                            ppl.yticks(fontsize=15)                            ppl.savefig('cl_OUTPUT/V-measure_' + mapname + '.png', dpi=200)                            # Silhouette method                            ppl.figure(figsize=(hsize, vsize))                            ppl.title("Silhouette: " + mapname, fontsize=16)                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                        y=km_silhouette[                                          Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                        s=circlesize, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                     km_silhouette[                                     Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                     color='r')                            ppl.grid(True)                            ppl.xlabel("Number of clusters", fontsize=14)                            ppl.ylabel("Silhouette score", fontsize=15)                            ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                       fontsize=14)                            ppl.yticks(fontsize=15)                            ppl.savefig('cl_OUTPUT/silhouette_' + mapname + '.png', dpi=200)                            # Davies-Bouldin score                            ppl.figure(figsize=(hsize, vsize))                            ppl.title("D-B Score: " + mapname, fontsize=16)                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                        y=db_score[                                          Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                        s=circlesize, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                     db_score[                                     Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                     color='r')                            ppl.grid(True)                            ppl.xlabel("Number of clusters", fontsize=14)                            ppl.ylabel("Davies-Bouldin score", fontsize=15)                            ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                       fontsize=14)                            ppl.yticks(fontsize=15)                            ppl.savefig('cl_OUTPUT/Davies-Bouldin_' + mapname + '.png', dpi=200)                            '''                            # BIC score with a Gaussian Mixture Model                            ppl.figure(figsize=(hsize, vsize))                            ppl.title("The Gaussian Mixture model BIC (red line) and AIC (blue line)", fontsize=16)                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                        y=np.log(                                            gm_bic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                        s=circlesize, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                     np.log(gm_bic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                     color='r')                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                        y=np.log(                                            gm_aic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                        s=circlesize, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                     np.log(gm_aic[Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters]),                                     color='b')                            ppl.grid(True)                            ppl.xlabel("Number of clusters", fontsize=14)                            ppl.ylabel("Log of Gaussian mixture BIC score", fontsize=15)                            ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                            ppl.yticks(fontsize=15)                            ppl.savefig('cl_OUTPUT/BIC(GMM)'+mapname+'.png', dpi=200)                            # (Log-)likelihood score                            ppl.figure(figsize=(hsize, vsize))                            ppl.title('Log-L-Score: '+ mapname, fontsize=16)                            ppl.scatter(x=[i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                        y=gm_score[                                          Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                        s=150, edgecolor='k')                            ppl.plot([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)],                                     gm_score[                                     Min_num_clusters_for_plot - Min_num_clusters:Max_num_clusters_for_plot - Min_num_clusters],                                     color='r')                            ppl.grid(True)                            ppl.xlabel("Number of clusters", fontsize=14)                            ppl.ylabel("Gaussian mixture GM_score", fontsize=15)                            ppl.xticks([i for i in range(Min_num_clusters_for_plot, Max_num_clusters_for_plot)], fontsize=14)                            ppl.yticks(fontsize=15)                            ppl.savefig('cl_OUTPUT/(Log-)Likelihood_score_' + mapname + '.png', dpi=200)                            '''                    cl_det(som, cl, mapname, plot_curves, degree)                ########## TESTING STAGE WITH TEST DATA RANGE ##########                # load test data                if Art_matrix is True:                    '''                    PATH = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/Art_matrices/new_matrices/ready/'                    fl = PATH + '*.csv'                    files = glob.glob(fl)                    FILE = []                    fbands = [x[(len(PATH)+4):-6] for x in files]  # removes everything before and after the frequency band info in the file name                    FILE = files[0][92:-4]  # removes ".csv"                    filename = PATH + FILE + '.csv'                    '''                    df = pd.read_csv(filename, header=None, skiprows=0)                    time_np = df[0]                    # convert time scale                    diffstart = date(1, 1, 1)                    diffend = date(1971, 1, 3)                    secs = (diffend - diffstart).total_seconds()                    for i in range(len(time_np)):                        if time_np[i] == 0:                            time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])                    new_time = time_np * 24 * 60 * 60 - secs                    time_np = []                    for d in range(len(new_time)):                        if new_time[d] < 0:                            gap = 0                            time_np.append(gap)                        else:                            time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))                    dates = list(time_np)                    newdates = [x[:-9] for x in dates]                    for row in newdates:                        if startdate_test == row:                            to_start = np.array(newdates.index(startdate_test))                            break                    for row in newdates:                        if enddate_test == row:                            to_end = np.array(newdates.index(enddate_test))                            break                    to_end = to_end - to_start                    '''                    df = pd.read_csv(filename, header=None, skiprows=(int(to_start)), nrows=(int(to_end)))                    time_np = df[0]                    # convert time scale                    diffstart = date(1, 1, 1)                    diffend = date(1971, 1, 3)                    secs = (diffend - diffstart).total_seconds()                    for i in range(len(time_np)):                        if time_np[i] == 0:                            time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])                    new_time = time_np * 24 * 60 * 60 - secs                    time_np = []                    for d in range(len(new_time)):                        if new_time[d] < 0:                            gap = 0                            time_np.append(gap)                        else:                            time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))                    dates = list(time_np)                    '''                else:                    df = pd.read_csv(filename, header=None, skiprows=1)                    time_np = df[0]                    dates = list(time_np)                    if len(dates[1])<11:                        newdates = dates                        for row in newdates:                            if startdate_test == row:                                to_start = np.array(newdates.index(startdate_test))                                break                        for row in newdates:                            if enddate_test == row:                                to_end = np.array(newdates.index(enddate_test))                                break                        to_end = to_end - to_start                    else:                        newdates = [x[:-9] for x in dates]                        for row in newdates:                            if startdate_test == row:                                to_start = np.array(newdates.index(startdate_test))                                break                        for row in newdates:                            if enddate_test == row:                                to_end = np.array(newdates.index(enddate_test))                                break                        to_end = to_end - to_start                df = pd.read_csv(filename, header=None, skiprows=(int(to_start)), nrows=(int(to_end)), low_memory=False)                dlen = df.shape[0]                time_np = df[0]                dfselection2 = pd.DataFrame(                    data=df.iloc[0:dlen, 1:df.shape[1]])                Testdata = dfselection2.values                # test SOM                #with open('/Users/bst/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/tested_maps/whakaari_043200.00wndw_rsam_10_2.00-5.00_data_features_5cl_5x5_2016-01-01_2020-01-01.pkl', 'rb') as f:  # Python 3: open(..., 'wb')                #    time_np, cloutput = pickle.load(f)                output = som.project_data(Testdata)                outputDF = pd.DataFrame(output)  # for each data, outputDF contains its assigned SOM node (bmu)                cl = som.cluster(n_clusters=n_clusters)                cloutput = cl[output]  # for each data, cloutput contains its assigned cluster                cloutputDF = pd.DataFrame(cloutput)                cloutputDF.to_csv('Clusters.csv', index=False, header=False)                if Art_matrix is True:                    time_np = df[0]                    diffstart = date(1, 1, 1)                    diffend = date(1971, 1, 3)                    secs = (diffend - diffstart).total_seconds()                    for i in range(len(time_np)):                        if time_np[i] == 0:                            time_np[i] = time_np[i - 1] + (time_np[i - 1] - time_np[i - 2])                    new_time = time_np * 24 * 60 * 60 - secs                    time_np = []                    for d in range(len(new_time)):                        if new_time[d] < 0:                            gap = 0                            time_np.append(gap)                        else:                            time_np.append(datetime.utcfromtimestamp(new_time[d]).strftime('%Y-%m-%d %H:%M:%S'))                else:                    time_np = df[0]                if not os.path.isdir('tested_maps'):                    os.makedirs('tested_maps')                if Art_matrix is True:                    with open('tested_maps/' + FILE + EXTNAME, 'wb') as f:  # Python 3: open(..., 'wb')                        pickle.dump([time_np, cloutput], f)                else:                    if extdrive is True:                        with open('/Volumes/TheBigOne/Data/Maps/2019 NO/tested_maps/' + FILE + EXTNAME,                                  'wb') as f:  # Python 3: open(..., 'wb')                            pickle.dump([time_np, cloutput], f)                    else:                        with open(                                '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/tested_maps/' + FILE + EXTNAME,                                'wb') as f:                            pickle.dump([time_np, cloutput], f)                ########### PLOTTING STAGE ###########                stationComp = 'WIZHHZ'  # just for the title                register_matplotlib_converters()  # to avoid warnings                if Art_matrix is True:                    with open('tested_maps/' + FILE + EXTNAME, 'rb') as f:  # Python 3: open(..., 'wb')                        time_np, cloutput = pickle.load(f)                else:                    if extdrive is True:                        with open('/Volumes/TheBigOne/Data/Maps/2019 NO/tested_maps/' + FILE + EXTNAME,                                  'rb') as f:  # Python 3: open(..., 'wb')                            time_np, cloutput = pickle.load(f)                    else:                        with open(                                '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/csv2som/tested_maps/' + FILE + EXTNAME,                                'rb') as f:                            time_np, cloutput = pickle.load(f)                time_np2 = []                for x in range(len(time_np)):                    try:                        time_np2.append(datetime.strptime(str(time_np[x]), '%Y-%m-%d %H:%M:%S'))                    except:                        time_np2.append(datetime.strptime(str(time_np[x]), '%Y-%m-%d'))                time_np = np.array(time_np2)                # plot initialization                fig, axarr = plt.subplots(1, sharex=True, sharey=False)                if interactive is False:                    fig.set_size_inches(16, 5)                axarr.set_yticklabels([])                axarr.axis('off')                ax = fig.add_subplot(1, 1, 1, sharex=fig.axes[0])                colormap = np.array(['r', 'purple', 'b', 'cyan', 'lime'])                tminimo = 0                tmassimo = time_np.shape[0]                colours = cloutput[tminimo:tmassimo] % colormap.shape[0]                ax.scatter(time_np[tminimo:tmassimo], cloutput[tminimo:tmassimo] + 1, c=colormap[colours])                ax.set_xlim(min(time_np), max(time_np))                ax.set_ylabel('Cluster number', fontsize = 15)                ax.grid(linestyle='dotted', which='both')                ax.yaxis.set_major_locator(MaxNLocator(integer=True))                # plt.pause(0.001)                fig.subplots_adjust(hspace=0.05)                plt.setp([a.get_xticklabels() for a in fig.axes[:-1]], visible=False)                # change interval of ticks on x-axis here depending on length of time-period covered by data                test_startdate = datetime.strptime('2010-12-30 00:00:00', '%Y-%m-%d %H:%M:%S')                if min(time_np) < test_startdate:                    ax.xaxis.set_major_locator(mdates.YearLocator(base=1, month=1, day=1, tz=None))                else:                    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=monthinterval))                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))                fig.autofmt_xdate()                plt.yticks(fontsize = 15)                plt.xticks(fontsize = 15)                # add events to plot                with open('act_log/eruptive_periods.txt', 'r') as fp:                    tes = [ln.rstrip() for ln in fp.readlines()]                xcoords = tes                for xc in xcoords:                    ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                with open('act_log/activity.txt', 'r') as fp:                    act = [ln.rstrip() for ln in fp.readlines()]                cords = act                for co in cords:                    ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                           label='ash emission')                ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                           label='observation of lava dome')                '''                # include earthquake events into classification plot                with open('act_log/tectonics.txt', 'r') as fp:                    tecs = [ln.rstrip() for ln in fp.readlines()]                xcoords = tecs                for xc in xcoords:                    ax.axvline(x=xc, color='green', linestyle='-', linewidth=3, label='_')                '''                # add legend and title                ax.legend(bbox_to_anchor=(0.25, 1), fontsize = 15, ncol=3)                TITLE = str(FILE + '   Cluster: ' + str(n_clusters) + '   SOM size: ' + str(mx[z]) + 'x' + str(my[z]))                #ppl.title(TITLE, loc='left')                # save plot                fig.set_size_inches(16, 6)                if not os.path.isdir('../__OUTPUT/plots'):                    os.makedirs('../__OUTPUT/plots')                if Art_matrix is True:                    path = '../__OUTPUT/plots/' + FILE + EXTNAME + '.png'                else:                    if extdrive is True:                        path = '/Volumes/TheBigOne/Data/Maps/2019 NO/plots/' + FILE + EXTNAME + '.png'                    else:                        path = '/Users/bste426/Documents/All/PhD/Data/Codes/SOM_Carniel/__OUTPUT/plots/' + FILE + EXTNAME + '.png'                plt.tight_layout()                plt.savefig(path, dpi=600)                '''                if interactive is True:                    fig.set_size_inches(15, 8)                    ppl.ion()                    ppl.show()                    ppl.pause(1000)                '''                plt.close()                if Art_matrix is True:                    if plot_features is True:                        # RSEM                        if not os.path.isdir('../__OUTPUT/plots/meanRSAM_plots'):                            os.makedirs('../__OUTPUT/plots/meanRSAM_plots')                        # df = pd.read_csv(filename, header=None, skiprows=0)                        meanRSAM = df[1]                        peakfreq = df[2]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, meanRSAM, '-', c='k', label='Energy')                        ax.set_xlabel('Time')                        ax.set_ylabel('RSEM')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                        plt.ylim(0, 0.00002)                        ppl.margins(x=0)                        ax.set_xlim(min(time_np), max(time_np))                        fig.autofmt_xdate()                            # add events to plot                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        path = '../__OUTPUT/plots/meanRSAM_plots/meanRSAM_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)                        # Dominant Frequency                        if not os.path.isdir('../__OUTPUT/plots/PeakFreq_plots'):                            os.makedirs('../__OUTPUT/plots/PeakFreq_plots')                        # df = pd.read_csv(filename, header=None, skiprows=0)                        meanRSAM = df[1]                        peakfreq = df[2]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, peakfreq, '-', c='r', label='Peak Frequency')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                            # add events to plot:                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        fig.autofmt_xdate()                        ax.set_xlabel('Time')                        ax.set_ylabel('peak frequency')                        plt.ylim(0, maxfreq)                        ax.set_xlim(min(time_np), max(time_np))                        ppl.margins(x=0)                        path = '../__OUTPUT/plots/PeakFreq_plots/PeakFreq_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)                        # Mean Frequency                        if not os.path.isdir('../__OUTPUT/plots/MeanFreq_plots'):                            os.makedirs('../__OUTPUT/plots/MeanFreq_plots')                        # df = pd.read_csv(filename, header=None, skiprows=0)                        meanfreq = df[3]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, meanfreq, '-', c='r', label='Mean Frequency')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                            # add events to plot                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        fig.autofmt_xdate()                        ax.set_xlim(min(time_np), max(time_np))                        ax.set_xlabel('Time')                        ax.set_ylabel('Mean frequency')                        plt.ylim(2, 5)                        ppl.margins(x=0)                        path = '../__OUTPUT/plots/MeanFreq_plots/MeanFreq_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)                        # Frequency Standard Deviation                        if not os.path.isdir('../__OUTPUT/plots/FreqSTD_plots'):                            os.makedirs('../__OUTPUT/plots/FreqSTD_plots')                            # df = pd.read_csv(filename, header=None, skiprows=0)                        freqstd = df[4]                        fig, ax = ppl.subplots(sharex=False, sharey=False)                        fig.set_size_inches(15, 5)                        ax.plot(time_np, freqstd, '-', c='r', label='Frequency STD')                        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))                            # add events to plot                        with open('act_log/eruptive_periods.txt', 'r') as fp:                            tes = [ln.rstrip() for ln in fp.readlines()]                        xcoords = tes                        for xc in xcoords:                            ax.axvline(x=xc, color='k', linestyle='-', linewidth=2, label='_')                        with open('act_log/activity.txt', 'r') as fp:                            act = [ln.rstrip() for ln in fp.readlines()]                        cords = act                        for co in cords:                            ax.axvline(x=co, color='dimgrey', linestyle='--', linewidth=2, label='_')                        ax.axvline(x='2012-08-04 16:52:00', color='k', linestyle='-', linewidth=2, label='eruption')                        ax.axvline(x='2012-09-02 00:00:00', color='dimgrey', linestyle='--', linewidth=2,                                   label='ash emission')                        ax.axvline(x='2012-11-24 00:00:00', color='dimgrey', linestyle=':', linewidth=2,                                   label='observation of lava dome')                        fig.autofmt_xdate()                        ax.set_xlim(min(time_np), max(time_np))                        ax.set_xlabel('Time')                        ax.set_ylabel('Frequency standard deviation')                        plt.ylim(0.5, 1)                        ppl.margins(x=0)                        path = '../__OUTPUT/plots/FreqSTD_plots/FreqSTD_' + FILE + EXTNAME + '.png'                        plt.savefig(path, dpi=200)########### CL_NUMBER ###########if CL_Determination is True:    #cl_mode = [i for i in cl_mode if i > 3]         # ideally have more than 3 clusters    CLM = pd.DataFrame(list(cl_mode))                # returns all cluster votes    #CLM = df/df.max()                               # normalisation    #CLM = pd.DataFrame(list(multimode(cl_mode)))    # returns only most frequent modes of clusters    if keep_CL_mode is True:        if os.path.isfile('CL_MODE_all.csv'):            CLM.to_csv('CL_MODE_{:s}.csv'.format(timewindow))            df = pd.read_csv('CL_MODE_{:s}.csv'.format(timewindow), index_col=0)            with open('CL_MODE_all.csv', 'a') as f:                df.to_csv(f, header=False)        else:            CLM.to_csv('CL_MODE_all.csv')            CLM.to_csv('CL_MODE_{:s}.csv'.format(timewindow))        #cl_fig = plt.plot        #cl_fig.set_size_inches(16, 9)        df = pd.read_csv('CL_MODE_all.csv', index_col=0)        df_new = df.rename(columns={'0': 'CL_Number'})        CL_fig = sns.displot(df_new, x="CL_Number", color='black', discrete=True, binwidth=1, stat="density")        #fig = CL_fig.get_figure()        CL_fig.savefig("CL_distribution.png", dpi=300)    else:        CLM.to_csv('CL_MODE_{:s}.csv'.format(timewindow))   # overwrites existing file - need to be saved under different name!        df = pd.read_csv('CL_MODE_{:s}.csv'.format(timewindow), index_col=0)        df_new = df.rename(columns={'0': 'CL_Number'})        CL_fig = sns.displot(df_new, x="CL_Number", color='black', discrete=True, binwidth=1, stat="density")        # fig = CL_fig.get_figure()        CL_fig.savefig("CL_distribution_{:s}.png".format(timewindow), dpi=300)